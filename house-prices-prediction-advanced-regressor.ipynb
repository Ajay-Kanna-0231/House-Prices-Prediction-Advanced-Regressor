{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\nprint(test_df.head)\nprint(test_df.columns)\nbackup_id = test_df['Id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Lasso, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Handle Missing Values\nfor col in train_df.columns:\n    if train_df[col].dtypes in [\"float64\", \"int64\"]:\n        if train_df[col].isnull().sum() > 0:\n            mean_value = train_df[col].mean()\n            train_df[col].fillna(mean_value, inplace=True)\n    elif train_df[col].dtypes == \"object\":\n        if train_df[col].isnull().sum() > 0:\n            mode_value = train_df[col].mode()[0]\n            train_df[col].fillna(mode_value, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Create new features in train_df\ntrain_df['TotalArea'] = (train_df['TotalBsmtSF'] + train_df['1stFlrSF'] +\n                         train_df['2ndFlrSF'] + train_df['GarageArea'] + \n                         train_df['WoodDeckSF'] + train_df['OpenPorchSF'] + \n                         train_df['EnclosedPorch'] + train_df['3SsnPorch'] + \n                         train_df['ScreenPorch'] + train_df['PoolArea'])\n\ntrain_df['TotalBath'] = (train_df['FullBath'] + 0.5 * train_df['HalfBath'] + \n                         train_df['BsmtFullBath'] + 0.5 * train_df['BsmtHalfBath'])\n\ntrain_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\ntrain_df['YearsSinceRemodel'] = train_df['YrSold'] - train_df['YearRemodAdd']\n\ntrain_df['OverallScore'] = train_df['OverallQual'] + train_df['OverallCond']\ntrain_df['ExterScore'] = train_df['ExterQual'] + train_df['ExterCond']\n\n# Drop columns used to create new features\ncolumns_to_drop = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea', 'WoodDeckSF', \n                   'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n                   'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', \n                   'YearBuilt', 'YearRemodAdd', 'OverallQual', 'OverallCond', \n                   'ExterQual', 'ExterCond']\n\ntrain_df.drop(columns=columns_to_drop, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-Hot Encoding for categorical variables\ntrain_df = pd.get_dummies(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\n# Step 1: Calculate mutual information\nX = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice']\n\n# Compute mutual information for each feature with respect to the target variable\nmi_scores = mutual_info_regression(X, y)\n\n# Convert the scores into a Series\nmi_scores = pd.Series(mi_scores, index=X.columns)\n\n# Sort the scores in descending order\nmi_scores = mi_scores.sort_values(ascending=False)\nprint(mi_scores)\n\n# Step 2: Identify low correlation features\n# Here, we define a threshold below which we consider the feature to have low correlation\nthreshold = 0.01\nlow_correlation_features = mi_scores[mi_scores < threshold].index.tolist()\n\n# Step 3: Drop low correlation features\ntrain_df.drop(columns=low_correlation_features, inplace=True)\n\n# The updated train_df now contains only the features with significant correlation (linear or non-linear) with the target variable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Step 3: Calculate correlation and drop columns with low correlation\ncorrelation_matrix = train_df.corr()\ntarget_correlation = correlation_matrix['SalePrice'].sort_values(ascending=False)\nlow_correlation = target_correlation[(target_correlation > -0.1) & (target_correlation < 0.1)]\ndrop_columns = low_correlation.index.tolist()\n\ntrain_df.drop(columns=drop_columns, inplace=True)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Prepare the test data with the same preprocessing steps\nfor col in test_df.columns:\n    if test_df[col].dtypes in [\"float64\", \"int64\"]:\n        if test_df[col].isnull().sum() > 0:\n            mean_value = test_df[col].mean()\n            test_df[col].fillna(mean_value, inplace=True)\n    elif test_df[col].dtypes == \"object\":\n        if test_df[col].isnull().sum() > 0:\n            mode_value = test_df[col].mode()[0]\n            test_df[col].fillna(mode_value, inplace=True)\n\n# Create new features in test_df\ntest_df['TotalArea'] = (test_df['TotalBsmtSF'] + test_df['1stFlrSF'] +\n                         test_df['2ndFlrSF'] + test_df['GarageArea'] + \n                         test_df['WoodDeckSF'] + test_df['OpenPorchSF'] + \n                         test_df['EnclosedPorch'] + test_df['3SsnPorch'] + \n                         test_df['ScreenPorch'] + test_df['PoolArea'])\n\ntest_df['TotalBath'] = (test_df['FullBath'] + 0.5 * test_df['HalfBath'] + \n                         test_df['BsmtFullBath'] + 0.5 * test_df['BsmtHalfBath'])\n\ntest_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\ntest_df['YearsSinceRemodel'] = test_df['YrSold'] - test_df['YearRemodAdd']\n\ntest_df['OverallScore'] = test_df['OverallQual'] + test_df['OverallCond']\ntest_df['ExterScore'] = test_df['ExterQual'] + test_df['ExterCond']\n\n# Drop columns used to create new features\ntest_df.drop(columns=columns_to_drop, inplace=True)\n\n# One-Hot Encoding for categorical variables in test_df\ntest_df = pd.get_dummies(test_df)\n\n# Align the test data with the train data\ntest_df = test_df.reindex(columns = train_df.columns, fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initializing models with pipelines for those needing normalization\nmodels = {\n    'Lasso': make_pipeline(StandardScaler(), Lasso()),\n    'Ridge': make_pipeline(StandardScaler(), Ridge()),\n    'SVR': make_pipeline(StandardScaler(), SVR()),\n    'GradientBoosting': GradientBoostingRegressor(),\n    'XGBoost': xgb.XGBRegressor(),\n    'LightGBM': lgb.LGBMRegressor(),\n    'RandomForest': RandomForestRegressor()\n}\n\n# Continue with the rest of your script\nmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\nresults = {}\nfor name, model in models.items():\n    mse_scores = cross_val_score(model, X_train, y_train, scoring=mse_scorer, cv=5)\n    rmse_scores = np.sqrt(-mse_scores)\n    results[name] = rmse_scores.mean()\n    print(f'{name} - CV RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std():.4f})')\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Train the best model on the entire training set and predict for the test set\nbest_model_name = min(results, key=results.get)\nbest_model = models[best_model_name]\n\n# Define hyperparameter grids for the best model\nparam_grids = {\n    'Lasso': {\n        'alpha': [0.01, 0.1, 1],\n        'max_iter': [5000],\n        'tol': [0.001]\n    },\n    'Ridge': {\n        'alpha': [0.1, 1, 10],\n        'solver': ['auto', 'sag']\n    },\n    'SVR': {\n        'C': [1, 10],\n        'kernel': ['rbf'],\n        'gamma': ['scale'],\n        'epsilon': [0.01, 0.1]\n    },\n    'GradientBoosting': {\n        'n_estimators': [100, 200],\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5],\n        'subsample': [0.9]\n    },\n    'XGBoost': {\n        'n_estimators': [100, 200],\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5],\n        'subsample': [0.9],\n        'colsample_bytree': [0.9]\n    },\n    'LightGBM': {\n        'n_estimators': [100, 200],\n        'learning_rate': [0.01, 0.1],\n        'num_leaves': [31, 50],\n        'subsample': [0.9],\n        'colsample_bytree': [0.9]\n    },\n    'RandomForest': {\n        'n_estimators': [100, 200],\n        'max_depth': [10, 20],\n        'min_samples_split': [2, 5],\n        'min_samples_leaf': [1, 2],\n        'bootstrap': [True, False]\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform GridSearchCV to find the best hyperparameters for the best model\nparam_grid = param_grids[best_model_name]\ngrid_search = GridSearchCV(best_model, param_grid, scoring='neg_mean_squared_error', cv=5)\ngrid_search.fit(X_train, y_train);\nbest_params = grid_search.best_params_\nprint(f'Best parameters for {best_model_name}: {best_params}')\n\n# Train the best model with the best parameters on the entire training set\nbest_model.set_params(**best_params);\nbest_model.fit(X_train, y_train);\n\n# Predict for the test set\ny_test_pred = best_model.predict(X_test);","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Fit the pipeline to the training data\nbest_model.fit(X_train, y_train);\n\nX_test_columns = set(X_test.columns)\n\n# Columns in test_df\ntest_df_columns = set(test_df.columns)\n\n# Find the extra column in test_df that is not in X_test\nextra_columns_in_test_df = test_df_columns - X_test_columns\nprint(extra_columns_in_test_df)\nprint(test_df.columns)\n\n\n# Directly use `predict` on the pipeline to predict on the test data\ntest_predictions = best_model.predict(test_df.drop(columns = [\"SalePrice\"]))  # Ensure to drop non-feature columns\n\n\n# Preparing the submission file\nsubmission = pd.DataFrame({\n    'Id': backup_id,\n    'SalePrice': test_predictions\n})\nsubmission.to_csv('submission.csv', index=False)\npd.read_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}